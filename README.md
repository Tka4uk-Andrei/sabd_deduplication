# sabd_deduplication

- [Brief project description](#brief-project-description)
- [Detailed project description (russian)](#detailed-project-description-russian)
- [Описание программной реализации](#описание-программной-реализации)
  - [Описание используемого стека](#описание-используемого-стека)
  - [Архитектуры системы](#архитектуры-системы)
  - [Описание структуры и работы программы](#описание-структуры-и-работы-программы)
  - [Описание работы с БД](#описание-работы-с-бд)
- [Тестирование](#тестирование)
- [Результаты тестирования](#результаты-тестирования)
  - [Скорость работы алгоритма](#скорость-работы-алгоритма)
  - [Прогресс добавления новых уникальных блоков](#прогресс-добавления-новых-уникальных-блоков)
  - [Количество переиспользуемых блоков](#количество-переиспользуемых-блоков)
  - [Количество одинаковых новых блоков внутри одного файла](#количество-одинаковых-новых-блоков-внутри-одного-файла)
  - [Количество ошибок кодирования](#количество-ошибок-кодирования)
- [Выводы](#выводы)

## Brief project description

This project contains simple (demo) application for data compression 
via dedublication process.

## Detailed project description (russian)

__Тип работы__: Курсовой проект

__Название работы__: Создание сервиса для дедупликации данных в хранилище

__Краткое описание__:\
Данный курсовой проект реализован в рамках дисциплины САБД ([описание дисциплины](https://github.com/Tka4uk-Andrei/semesters_description/blob/master/semester_3.md#системы-анализа-больших-данных))
В рамках данного проекта требуется выполнить следующию задачу:

> Цель работы состоит в реализации системы оптимального хранения
данных за счет использования подхода дедупликации данных и проведении
тестирования для измерения производительности созданного прототипа.

Для достижения потавленной цели требуется:
- изучить подходы к оптимизации хранения данных в
традиционных базах данных;
- выбрать стек технологий, необходимый для создания прототипа
системы дедупликации;
- разработать систему оптимального хранения данных на диске;
- провести нагрузочное тестирование и установить зависимость
скорости чтения/записи данных в локальное хранилище в
зависимости от размера сегмента;
- установить оптимальный размер сегмента блока данных, при
котором наблюдается:
    - максимальная скорость записи
    - максимальная скорость чтения
    - процент ошибки восстановления данных в зависимости от
      использованного алгоритма hash.

Система оптимального хранения содержит следующие компоненты:
- Источник данных
- Генератор дедуплицированных данных
- Хранилище дедуплицированных данных
- Хранилище таблицы сопоставляющей хеш и его данные, из которых он получен

## Описание программной реализации
### Описание используемого стека
В качестве языка разработки был выбран Python 3. Данный ЯП прекрасно подходит 
для создания небольших прототипов, а также для него написано множество библиотек, 
которые удобно подключать.

Для хранения таблицы сопоставляющей хеш и значения используется sqlite. Данная БД
легковесна и не требует больших усилий по развёртыванию.

В качестве входных данных используются музыкальные mp3 файлы размером 6-8 мегабайт.
Использование такого вида входных данных позволяет упростить систему, а именно не 
требуется реализовывать модуль по нарезке данных на одинаковые фргменты.

### Архитектуры системы
Графически архитектура системы выглядит следующим образом.

![Архитектура системы](img/architecture.png)

Поэтапно работу программы можно описать следующим образом:
- Кодирование
  1. Получение списка файлов для сжатия
  2. Каждый файл читается как массив байт
  3. Каждый массив нарезается на блоки размера `SEGMENT_SIZE`
  4. Для каждого блока вычисляется хеш и идентификационный номер. Результаты записываюся в массивы.
     Новые хеши вносятся в базу данных
  5. Каждый массив записывается в сжатые файлы
- Декодирование
  1. Получение списка файлов для восстановление
  2. Получение от БД данных о 
  3. Чтение каждого файла из списка. По каждой ячейке восстанавливается исходный блок данных.
     Блоки вносяться в массив
  4. Массивы с исходными блоками записываются в файлы.

### Описание структуры и работы программы
Программа разбита на три основные функции:
- compress_data - функция отвечающая за сжатие данных и запись блоков с их 
  идентификаторами в БД
- decompress_data - функция отвечающая за восстановление данных на основании 
  данных их БД и сжатого файла
- data_compare - функция сопоставляющая исходные байты файла с востановленными.
  Важно, что каждый отличающийся байт считается ошибочным.

Работа основных функций опирается на следующие вспомогательные функции:

| Название функции         | Описание                                           |
| ------------------------ | -------------------------------------------------- |
| get_file_names_in_folder | Функция возвращает имена файлов в заданной папке |
| get_hash_for_segment     | Вычисляет хеш для переданного массива байт |
| get_segmented_sequence   | Нарезает исходный файл на массивы байт одного размера |
| binary_file_read         | Читает заданный файл как двоичный. Возвращает массив байт |
| binary_file_write        | Запись массива байт в заданный файл. |
| save_compresed_data_file | Запись массива из идентификаторов хешей в файл. Идентификаторы записываются блоками из байт одинакового размера |
| read_compresed_data_file | Читает сжатый файл. В результате работы функция возвращает исходный массив байт |
| get_data_from_bd         | Получение списка, содержащего все поля из таблицы. |
| insert_hashes_into_bd    | Запись данных в таблицу |
| update_hashes_in_bd      | Обновление данных в таблице (нужна только для обновления количества переиспользований) |

Также есть две функции для тестирования системы:
| Название функции    | Описание                                           |
| ------------------- | -------------------------------------------------- |
| run_compress_test   | Запуск тестового сценария на сжатие данных         |
| run_decompress_test | Запуск тестового сценария на восстановление данных |

Работой программы можно управлять с помощью параметров, объявленных в начале программы. 
Названия и описаня приведены в таблице ниже.
| Название | Возможные значения | Описание |
|---|---|---|
| DATA_PATH                    | Строка | Путь к папке с исходными данными |
| SEGMENT_SIZE                 | Целое положительное число | Размер блока для сжатия (байты) |
| ZIPPED_DATA_CELL_SIZE        | Целое положительное число | Размер блока выделяемого под идентификаторы хешей |
| HASH_FUNCTION_IN_USE         | "None" или "sha1" или "MD5" | Хеш-функция для сжатия |
| FOLDER_WITH_COMPRESED_DATA   | Строка | Путь к папке со сжатыми данными |
| FOLDER_WITH_DECOMPRESED_DATA | Строка | Путь к папке с востановленными файлами |
| BD_FILE_NAME                 | Строка | Название файла, где храниться БД |

Важно отметить, что если размер `ZIPPED_DATA_CELL_SIZE` будет слишком мал, а именно максимальное 
число вариантов вариантов будет превышать число, которое может вмеместить `ZIPPED_DATA_CELL_SIZE`,
то программа аварийно завершиться.

### Описание работы с БД
База данных состоит из одной таблицы, которая содержит поля приведённые в 
таблице ниже:

| Имя поля  | тип       | описание |
| --------- | --------- | -------- |
| id        | id        | Идентификационный номер хеша. Записывается в сжатый файл |
| hash      | hash      | Хеш-сумма вычисленная для блока данных заданого размера  |
| hash_val  | hash_val  | Значение блока, используемое для вычисления хеш функции  |
| rep_count | rep_count | Количество переиспользований                             |


## Тестирование
Для тестирования использовалось 8 mp3 файлов от 6 до 9 мегабайт каждый. Работа 
программы тестировалась со всеми тремя видами сжатия. В файлах сжимались блоки 
следующих размеров:
- 100 байт
- 50 байт
- 20 байт
- 10 байт, только для `None` алгоритма сжатия
- 4 байт, только для `None` алгритма сжатия

Для получения значений среднего времени работы программы, какждое тестовое 
условие (пара хеш алгоритм и сжимаемый блок) запускалось 100 раз.

## Результаты тестирования
В данном разделе приведены в виде графиков результаты тестирования. 
Сравнивается:
- динамика среднего времени работы для отдельных входных файлов
- количество добавленных уникальных блоков данных
- количество переиспользованных блоков относительно старых файлов
- количество одинаковых новых блоков для тестируемого файла
- количество ошибок кодирования/декодирования

### Скорость работы алгоритма сжатия

*Примечание.* Время работы приведено для каждого отделного файла.

Скорость для блока в 100 байт

![скорость для блока в 100 байт](/img/avg_time_compress_100.png)

Скорость для блока в 50 байт

![скорость для блока в 50 байт](/img/avg_time_compress_50.png)

Скорость для блока в 20 байт

![скорость для блока в 20 байт](/img/avg_time_compress_20.png)

Скорость для блока в 10 и 4 байт

![скорость для блока в 10 и 4 байт](/img/avg_time_compress_10_and_4.png)

Сопоставление скорости работы между всеми вариантами

![Сопоставление скорости работы между всеми вариантами](/img/avg_time_compress_all.png)

---

### Скорость работы алгоритма восстановления

*Примечание.* Время работы приведено для каждого отделного файла.

Скорость для блока в 100 байт

![скорость для блока в 100 байт](/img/avg_time_decompress_100.png)

Скорость для блока в 50 байт

![скорость для блока в 50 байт](/img/avg_time_decompress_50.png)

Скорость для блока в 20 байт

![скорость для блока в 20 байт](/img/avg_time_decompress_20.png)

Скорость для блока в 10 и 4 байт

![скорость для блока в 10 и 4 байт](/img/avg_time_decompress_10_and_4.png)

Сопоставление скорости работы между всеми вариантами

![Сопоставление скорости работы между всеми вариантами](/img/avg_time_decompress_all.png)

---

### Прогресс добавления новых уникальных блоков

Добавление для блока в 100 байт

![Добавление для блока в 100 байт](/img/hash_inc_amount_100.png)

Добавление для блока в 50 байт

![Добавление для блока в 50 байт](/img/hash_inc_amount_50.png)

Добавление для блока в 20 байт

![Добавление для блока в 20 байт](/img/hash_inc_amount_20.png)

Добавление для блока в 10 и 4 байт

![Добавление для блока в 10 и 4 байт](/img/hash_inc_amount_10_and_4.png)

Сопоставление добавления между всеми вариантами

![Сопоставление добавления между всеми вариантами](/img/hash_inc_amount_all.png)

---

### Количество переиспользуемых блоков

Количество переиспользований для блока в 100 байт

![Количество переиспользований для блока в 100 байт](/img/reused_hashes_100.png)

Количество переиспользований для блока в 50 байт

![Количество переиспользований для блока в 50 байт](/img/reused_hashes_50.png)

Количество переиспользований для блока в 20 байт

![Количество переиспользований для блока в 20 байт](/img/reused_hashes_20.png)

Количество переиспользований для блока в 10 и 4 байт

![Количество переиспользований для блока в 10 и 4 байт](/img/reused_hashes_10_and_4.png)

Количество переиспользований между всеми вариантами

![Количество переиспользований между всеми вариантами](/img/reused_hashes_all.png)

---

### Количество одинаковых новых блоков внутри одного файла

Количество одинаковых новых блоков в 100 байт

![Количество одинаковых новых блоков в 100 байт](/img/dublicated_amount_100.png)

Количество одинаковых новых блоков в 50 байт

![Количество одинаковых новых блоков в 50 байт](/img/dublicated_amount_50.png)

Количество одинаковых новых блоков в 20 байт

![Количество одинаковых новых блоков в 20 байт](/img/dublicated_amount_20.png)

Количество одинаковых новых блоков в 10 и 4 байт

![Количество одинаковых новых блоков в 10 и 4 байт](/img/dublicated_amount_10_and_4.png)

Количество одинаковых новых блоков между всеми вариантами

![Количество одинаковых новых блоков между всеми вариантами](/img/dublicated_amount_all.png)

---

### Количество ошибок кодирования
В ходе работы программы во всех вариантах ошибок при сжатии/востановлении 
не было.

## Выводы
В результате работы был создан прототип простейшей системы сжатия данных.

По результатам тестирования можно сделать следующие выводы:
- Скорость работы программы по сжатию файлов линейно зависит от количества 
  обработанных файлов до этого.
- Чем больше размер блока, тем меньше время работы программы
- Время восстановления файла постоянно, скорость работы также как в сжатии 
  зависит от размера блока, чем он больше, тем меньше время
- Количество одинаковых блоков внутри одного файла только для случая с 
  размером блока 4 байта, во всех остальных случаях оно стагнирует.
- Добавление новых хешей происходит величины одного порядка не зависисмо от 
  количества файлов. Для мешьшего размера блока большее число новых хешей

В текущей системе можно сделать следующие улучшения:
- Добавить функцию нарезающую/объединяющую входные файлы на блоки одного 
  размера. Данная возможность нужна для обработки больших файлов или большого 
  набора очень маленьких. При использовании 
- Добавка поддержки многопоточной обработки данных 
- Попробовать другие способы хранения блоков данных.  хранение с помощью
  обычных реляционных баз данных может в будущем стать узким местом системы.
  Также, если при расчёте объёма сжатых данных учитывать размер БД, то с текущим 
  набором данных, то сжатия не происходит.

